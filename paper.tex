\documentclass{IEEEtran}

\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{colortbl} % for \rowcolor
\usepackage{caption}
\usepackage{subcaption}
\usepackage[bookmarks=false]{hyperref}
\usepackage{algorithm}
\usepackage{algorithmicx} % Doc is at http://tug.ctan.org/macros/latex/contrib/algorithmicx/algorithmicx.pdf
\usepackage{algpseudocode}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=blue,filecolor=black,urlcolor=blue}

\newcommand{\todo}[1]{\marginpar{\parbox{18mm}{\flushleft\tiny\color{red}\textbf{TODO}:
      #1}}}
\definecolor{headcolor}{gray}{0.9}

\algblock{Input}{EndInput}
\algnotext{EndInput}
\algblock{Output}{EndOutput}
\algnotext{EndOutput}
\newcommand{\Desc}[2]{\State \makebox[2em][l]{#1}#2}

\begin{document}

\title{Performance Evaluation of Big Data Processing Strategies for Neuroimaging}

\author{
  \IEEEauthorblockN{
    Val\'erie Hayot-Sasson$^1$, Shawn T Brown$^2$ and 
    Tristan Glatard$^1$
  }\\
  \IEEEauthorblockA{
    $^1$Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada\\
    $^2$Montreal Neurological Institute, McGill University, Montreal, Canada
  }
}

\maketitle

\begin{abstract}
    Neuroimaging datasets are rapidly growing in size as a result of 
    advancements in image acquisition methods, open-science and data sharing. 
    However, it remains that current neuroimaging data processing engines do 
    not employ strategies adopted by Big Data processing engines to improve 
    Big Data processing time. Here, we 
    evaluate three Big Data processing stategies (in-memory computing, 
    data-locality and lazy-evaluation) on typical neuroimaging use 
    cases, represented by the BigBrain dataset. We contrast the various 
    processing strategies by using Apache Spark and Nipype, as our 
    representative Big Data and neuroimaging processing engines, on Dell EMC's 
    Top-500 cluster. 
    Big Data thresholds were modeled by comparing the data/compute ratio of the 
    application to the filesystem/concurrent processes ratio. 
    This model acknowledges the 
fact that page caching provided by the Linux kernel is critical to the 
performance of Big Data applications. Results show that in-memory 
computing alone speeds-up executions by a factor of up to 1.6, whereas 
when combined with data locality, this factor reaches 5.3. Such important 
speed-up values are likely to be observed on typical image processing 
operations performed on images of size larger than 75GB. A ballpark 
speculation from our model showed that in-memory computing alone will
not speed-up current 
functional MRI analyses unless coupled with data 
locality, with a number of concurrently processed subject in the  280 ballpark. 
    In addition, we observe that emulating in-memory computing 
using in-memory file systems (tmpfs) does not reach the performance of 
an in-memory engine, presumably due to swapping to disk and the lack of data cleanup.
We conclude that Big Data processing strategies are 
worth developing for neuroimaging 
applications. 
\end{abstract}

\section{Introduction} % 1 page with abstract

Big Data processing engines reduce data-related overheads through 
use of concepts such as data locality, in-memory computing, and lazy evaluation.
Data locality-aware scheduling, popularized by the MapReduce~\cite{dean2008mapreduce} 
framework, executes tasks in the nodes holding the data rather than 
performing costly data transfers across the network. In-memory 
computing, adopted by MapReduce's successor Apache 
Spark~\cite{zaharia2016apache}, stores data in memory whenever 
possible, reducing costly disk I/Os between pipeline steps. 
Lazy evaluation, also used in Spark, enables further Big Data performance 
optimizations, by evaluating results only when necessary. Frameworks 
such as
MapReduce and Spark have become mainstream tools for data analytics, 
although many others, such as 
Dask~\cite{rocklin2015dask}, are emerging. 
Meanwhile, several scientific 
domains including bioinformatics, physics or astronomy, have entered 
the Big Data era due to increasing data volumes and variety. 
Nevertheless, the adoption of Big Data engines for scientific data analysis 
remains limited, perhaps due to the widespread availability of 
scientific processing engines such as Pegasus~\cite{deelman2005pegasus} or
Taverna~\cite{oinn2004taverna}, and the adaptations required in Big 
Data processing engines for scientific computing. 

Scientific applications differ from typical Big Data use 
cases, which might explain the remaining gap between Big Data and 
scientific engines. While Big Data applications mostly target text 
processing (e.g. Web search, frequent pattern mining, recommender 
systems~\cite{leskovec2014mining}) implemented in consistent software 
libraries, scientific applications often involve 
binary data such as images and signals, processed by a sequence of 
command-line/containerized tools
using a mix of programming languages (C, Fortran, Python, shell 
scripts), referred to as workflows or pipelines. Infrastructure-wise, 
Big Data applications commonly run on 
clouds or dedicated commodity clusters with locality-aware file systems 
such as the Hadoop Distributed File System 
(HDFS~\cite{shvachko2010hadoop}), whereas scientific applications are 
usually deployed on large, shared clusters where data is moved between
data nodes and compute nodes using shared file systems such 
as Lustre~\cite{schwan2003lustre}. Such differences in applications and 
infrastructures have important consequences. To 
mention only one, in-memory computing requires instrumentation to be 
applied to command-line tools. 

Technological advances of the past decade, in particular page caching 
in the Linux kernel~\cite{love2010linux}, in-memory file systems 
(tmpfs) and memory-mapped files might also 
explain the lack of adoption of Big Data engines for scientific 
applications. In such configurations, in-memory computing would be a feature 
provided by 
the operating system rather than by the engine itself. The frontier 
between these two components is blurred and needs to be clarified.

% https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20130001600.pdf

Our primary field of interest, Neuroimaging, is no exception to the 
generalized rise of data volumes in science due to the joint increase 
of image resolution and subject cohort sizes~\cite{van2014human}. 
Processing engines have been developed with neuroinformatics 
applications in mind, for instance Nipype~\cite{gorgolewski2011nipype} 
or the Pipeline System for Octave and Matlab 
(PSOM~\cite{bellec2012pipeline}). Big Data engines have been used for 
neuroimaging applications too, including in the Thunder 
project~\cite{freeman2014mapping} and in more specific works such 
as~\cite{makkie2019fast}. However, no quantitative performance 
evaluation has been conducted on neuroimaging applications to assess the 
added-value of Big Data engines compared to traditional processing engines.

This paper addresses the following questions:
\begin{enumerate}
\item What is the effect of in-memory computing, lazy evaluation and data locality on current neuroimaging applications?
\item Can in-memory computing be effectively enabled by the operating system rather than the data processing engine?
\end{enumerate}

Answers to these questions have important implications. 
In~\cite{mehta2017comparative}, a comparative study of Dask, Spark, 
TensorFlow, MyriaDB, and SciDB on neuroinformatics use-cases is 
presented. It concludes that these systems need to be extended to 
better address scientific code integration, data partitioning, data 
formats and system tuning. We argue 
that such efforts should only be conducted if substantial performance 
improvements are expected from in-memory computing, lazy 
evaluation or data 
locality. On the other hand, neuroimaging data processing engines are 
still being developed, and the question remains whether these 
projects should just be migrated to Spark, Dask, or other Big Data 
engines.

Our study focuses on performance. We intentionally do not 
compare Big Data and scientific data processing engines on the grounds 
of workflow language expressivity, fault-tolerance, provenance capture 
and representation, portability or reproducibility which are otherwise 
critical concerns, addressed for instance in~\cite{samba}. Besides, our 
study of performance focuses on the impact of data writes and 
transfers. It purposely leaves out task scheduling to computing 
resources, to focus on the understanding of 
data writes and movement. Task scheduling will be part of our 
discussion, however.

Infrastructure-wise, we focus on the case of High-Performance Computing 
(HPC) clusters that are typically available through University 
facilities or national computing infrastructures such as 
\href{xsede.org}{XSEDE}, \href{http://computecanada.ca}{Compute Canada} 
or \href{http://www.prace-ri.eu}{PRACE}, as neuroscientists typically use such platforms.
 We assume that HPC systems are 
multi-tenant, that compute nodes are accessible through a batch 
scheduler, and that a file system shared among the compute nodes is 
available. We intentionally did not consider distributed, 
HDFS-like file systems, as initiatives to deploy them in HPC 
centers, for instance Hadoop on-demand~\cite{krishnan2011myhadoop}, have not 
become mainstream yet.

%\todo{Perhaps talk about modeling and simulation here.}

Our methods, including performance models, processing engines, 
applications and infrastructure used, are described in 
Section~\ref{sec:methods}. Section~\ref{sec:results} presents our 
results which we discuss in Section~\ref{sec:discussion} along with the 
two research questions mentioned previously. 
Section~\ref{sec:conclusion} concludes on the relevance of Big Data 
processing strategies for neuroimaging applications.

\section{Materials and Methods} % 4 pages
\label{sec:methods}

The application pipelines, benchmarks, performance data, and analysis scripts used 
to implement the methods described hereafter are all available at 
\url{https://github.com/big-data-lab-team/paper-in-mem-locality} for 
further inspection and reproducibility. Links to the processing engines 
and processed data are provided in the text.

\subsection{Engines} % 1.5 pages

\subsubsection{Apache Spark}

Apache Spark is a popular Scala-based processing framework for Big Data, with
APIs in Java, Python and R. Its 
generalized nature allows Spark to not only be applied to batch workflows,
but also SQL queries, iterative machine learning applications, 
data streaming and graph processing. Spark's
main features include data-locality, in-memory processing and lazy evaluation,
which it achieves through its principle abstraction, the Resilient Distributed 
Dataset (RDD). 

An RDD is an immutable parallel data structure that achieves fault-tolerance 
through the concept of lineage~\cite{zaharia2010spark}. Rather than permitting
fine-grained transformations, only coarse-grained transformations (applying to
many elements in the RDD), can be applied, thereby making it simple to maintain a 
log of data modifications. This log, known as the lineage, is used
to reproduce any lost data.

Two types of operations can be performed on RDDs:
transformations and actions. Applying a transformation to an RDD produces a new
child RDD through a narrow or wide dependency. A narrow dependency signifies 
that the child is only dependent on a single parent partition, whereas a child 
RDD is dependent on all parent partitions in a wide dependency. Examples of 
Spark transformations include map, filter and join. To materialize an RDD, an
action must be performed, such as a reduce or a collect. Lazy evaluation is 
represented in Spark through the use of transformations and actions. A series of
transformations may be defined without the data ever being materialized. Using 
this strategy, Spark can optimize data processing
throughout the application.

All actions and wide dependencies require a shuffle -- Spark's most costly
operation. Every shuffle begins with each map task saving its data to local
files for fault tolerance. The shuffle operation then 
redistributes the data across the partitions as requested. A shuffle marks a 
stage boundary in Spark. In other words, an reduce operation  will not begin 
until all 
dependent map tasks have completed.

Spark will spill RDD data to disk whenever the complete dataset does not fit in
memory. Moreover, as Spark transformations generate new RDDs and numerous 
transformations may occur in a single application, Spark implements a Least-Recently Used (LRU) eviction
policy. If an evicted RDD needs to be reused, Spark will recompute it using the
lineage data collected. As demonstrated in~\cite{freeman2014mapping}, caching significantly improves processing times of iterative algorithms where 
RDDs are reused. It can be of even greater importance if the RDD is costly to 
recompute.

Data locality in Spark is achieved through the scheduling of tasks to partitions 
which have the
data loaded in memory. If the data is instead stored on HDFS, the scheduler will
assign it to one of the preferred locations specified by HDFS. Spark's scheduler
utilizes delay scheduling to optimize fairness and locality for all tasks.
%improve paragraph

Three different types of schedulers are compatible with Spark: 1) Spark 
Standalone, 2) YARN~\cite{vavilapalli2013apache} and 3) Mesos~\cite{hindman2011mesos}. The Spark Standalone
scheduler is the default scheduler. YARN was a scheduler designed for Hadoop~\cite{white2012hadoop}
clusters and is prepackaged with Hadoop installations. In contrast to YARN, 
Mesos was designed to be used in multitenant cluster environments. In our experiments,
we focus on the Spark Standalone cluster.

Executing Spark applications on HPC systems with Spark-unaware schedulers may
be inefficient. The amount of resources requested by Spark may impede 
Spark-cluster scheduling time. Using pilot-scheduling strategies to add 
nodes to the Spark cluster as they are allocated by the underlying 
schedulers may speedup allocation and overall processing time~\cite{paraskevakos2018pilot}. This, however, is not studied in the 
current paper.

As Spark is frequently used by the scientific community, we designed 
our experiments using the PySpark API. This is at a cost to performance 
as the PySpark code must undergo Python to Java serialization. We used Spark
version 2.3.2 installed from \url{https://spark.apache.org}.

\subsubsection{Nipype}

% i use 'popular' for both... widely-used (for Spark)?
Nipype is a popular Python neuroimaging processing engine. It aims at being a 
solution for easily creating reproducible neuroimaging workflows. Although Nipype
does not employ any Big Data processing strategies, it provides access to numerous
plugins to schedulers found in most clusters readily available to researchers, 
such as the SunGrid Engine (SGE), Portable Batch System (PBS), Slurm and HTCondor\todo{cite}. It also includes its own scheduler, MultiProc,
for parallel processing on single nodes. Nipype also includes many built-in 
interfaces to commonly used neuroimaging tools to be incorporated within the 
workflows. Nipype's ability to easily parallelize workflows in researcher-available 
cluster setups, capture detailed provenance information necessary for reproducibility,
and allow users to easily integrate existing neuroimaging tools, make it preferable
to existing Big Data solutions, which would necessitate modification to achieve this.

Jobs, or Interfaces, in Nipype, are encapsulated by a Node. A Node dictates that
the job will execute on a single input. However, the MapNode, 
a child variant of the Node, can execute on multiple inputs. All tasks in
Nipype execute in their own uniquely named subdirectory which facilitates provenance 
tracking of inputs and outputs, and also enables checkpointing of the workflow.
In the case of Node failure or application modification, only the nodes who have
been modified (verified by hash) are re-executed.

In order for Nipype to operate as intended, a filesystem shared by all the nodes 
is required. However, it is still possible to save to a non-shared 
local filesystem, but it may come at the expense of fault-tolerance, as 
a result of being stored in a disk-local file system \todo{I don't get 'as a result of being stored in a disk-local file system'}. Moreover, the 
user will need to ensure that the files are appropriately directed to 
the nodes that require them as there is no guarantee of data locality 
in Nipype.

We used Nipype version 1.1.4 installed through the pip package manager.

\subsection{Data Storage Locations}

Data storage location is critical to the performance of Big Data 
applications on HPC clusters.
Data may reside in the engine memory, on 
a file system whose content reside in virtual memory (for instance 
tmpfs), on disks local to the processing node, or on a shared 
file system. Table~\ref{table:features} summarizes the Big Data 
strategies that can be used depending on the data location. In 
addition, lazy evaluation is available in Spark regardless of data 
location. The remainder of this Section explains this Table and 
provides related performance models.
\begin{table}
\centering
\begin{tabular}{c|cc}
   \rowcolor{headcolor}
    Data Location                 & In-Memory     & Data Locality        \\
    \rowcolor{headcolor}
                                  & Computing     &                     \\
                                  \hline          
In-memory                         & \cellcolor{green!25} Yes           & \cellcolor{green!25}Yes                      \\
tmpfs                             & \cellcolor{green!25} Yes           & \cellcolor{green!25}Yes                  \\
Local Disk                        & \cellcolor{orange!25}Page Caching  & \cellcolor{green!25}Yes                  \\
Shared File System                & \cellcolor{orange!25}Page Caching  & \cellcolor{red!25}No                
\end{tabular}
\caption{Big Data strategies on a shared HPC cluster.}
\label{table:features}
\end{table}

\subsubsection{In-Engine-Memory and In-Memory File System} 

 The main difference between storing the data in the engine memory, as in Spark, 
 and simply writing to an in-memory file system, such as tmpfs, is 
 what happens when the processed data fills up the available 
 memory. In case engine memory is used, the engine must cleanup 
 unused data with some strategy to avoid crashes. In case an in-memory 
 file system is used, the kernel starts swapping memory to disk, and 
 the performance becomes that of local disk writes. In our experiments, 
 we will explore the region  when the data consumed by the 
 application approaches the threshold of available memory.

\subsubsection{Local Disk} % 0.5 page

% Perhaps refer to this: https://ieeexplore.ieee.org/abstract/document/5496998

Storing data on local disks obviously enables data locality as tasks 
executed on the node where their input files reside do not require to 
transfer input or output data. However, in absence of a more specific 
file system such as HDFS to handle 
file replication across computing nodes, data locality comes at the price
of stringent scheduling restrictions, as tasks can only be scheduled to the
single node that contains their input data.


The performance of local disk accesses is strongly dependent on the 
page caching mechanism provided by the Linux kernel, described in    
details in~\cite{love2010linux}. To summarize, data read from disk 
remains cached in memory until evicted by an LRU (Least Recently Used) 
strategy. When a process invokes the \texttt{read()} system call, the 
kernel will return the data directly from memory if the requested data 
lies in the page cache, realizing a cache \emph{hit}. Cache hits drastically speed-up data 
reads, by masking the disk latency and bandwidth behind a 
memory buffer. In effect, page caching provides in-memory computing 
transparently to the processing engine. However, page cache eviction 
strategies currently cannot be controlled by the application, which 
prevents processing engines to anticipate reads by preloading the 
cache. Scheduling strategies might be designed 
to maximize cache hits though. For instance, lazy 
evaluation could result in more cache hits by scheduling data-dependent 
tasks on the same node.

Page caching has a more dramatic effect on disk writes, reducing their 
duration by several orders of magnitude. When a process calls the 
\texttt{write()} system call, data is copied to a memory cache that is 
asynchronously written to disk by flusher threads, when memory shrinks, when
``dirty" (unwritten) data grows, or when a 
process invokes the \texttt{sync()} system call. 
This asynchronous flushing of the page cache is called 
\emph{writeback}.
%~ It should be noted that writeback may lead to data 
%~ loss in case of system failure. 

Page caching is essentially a way to 
emulate in-memory computing at the kernel level, without requiring a 
dedicated engine. The size of the page cache, however, becomes a 
limitation when processes write faster than the disk bandwidth. When 
this happens, the page cache rapidly fills up and writes are limited by 
the disk write bandwidth as if no page cache was involved.

We introduce the following basic model to describe the filling and 
flushing of the page cache by an application:
$$
d(t) = \left( \frac{D}{C} - \frac{\delta}{\gamma} \right)t + d_0,
$$
where:
\begin{itemize}
\item $d(t)$ is the amount of data in the page cache at time t
\item $D$ is the total amount of data written by the application
\item $C$ is the total CPU time of the application
\item $\delta$ is the disk bandwidth
\item $\gamma$ is the max number of concurrent processes on a node
\item $d_0$ is the amount of data in the page cache at time $t_0$
\end{itemize}

This model applies to parallel applications assuming that (1) 
concurrent processes all write the same amount of data, (2) 
concurrent processes all consume the same CPU time, (3) data is written 
uniformly along task execution. With these assumptions, all the 
processes will write at the same rate, which explains why the model 
does not depend on the total number of concurrent processes in the 
application, but only on the max number of concurrent processes 
executing on the same node ($\gamma$). While these 
assumptions would usually be violated in practice, this simple 
model already provides interesting insights on the performance of disk 
writes, as shown later. Naturally, the model also ignores other 
processes that might be writing to disk concurrently to the 
application, which we assume negligible here. 

In general, an application should ensure that $\dot d$ remains negative 
or null, leading to the following inequality:
\begin{equation}
\frac{D}{C} \leq \frac{\delta}{\gamma} \label{eq:page-cache-inequality}
\end{equation}
This defines a data-compute ratio beyond which the page cache becomes 
asymptotically useless. It should be noted that the transient phase 
during which the page cache fills up might last a significant amount of time, 
in particular when $\dot d$ is positive and small. We intentionally do not model the 
transient phase as it requires detailed knowledge of difficult to estimate parameters  
such as the page cache and the initial amount 
of data in it ($d_0$).

 We will use Equation~\ref{eq:page-cache-inequality} to 
define our benchmarks and interpret the results. It 
should be noted that leveraging the page cache, and therefore ensuring 
that Equation~\ref{eq:page-cache-inequality} holds, has important 
performance implications: with page caching, the write throughput will 
be that of memory, while without page caching it will be that of the 
disk.

% Cache eviction: LRU/n.

\subsubsection{Shared File System}

We model a shared file system using its global apparent bandwidth 
$\Delta$, shared by all concurrent processes in the cluster. We are 
aware that such a simplistic model does not describe at all the 
intricacies of systems such as Lustre. In particular, metadata 
management, RPC protocol optimizations and storage optimizations are 
all covered under the apparent bandwidth. We do, however, consider the 
effect of page caching in shared file systems too, since in Linux 
writes to network-mounted volumes benefit from this feature too.

As in the local disk model, we note that page caching will only be 
useful when the flush bandwidth is greater than the write throughput of 
the application, that is:
\begin{equation}
\frac{D}{C} \leq \frac{\Delta}{\Gamma}, \label{eq:page-cache-sharedfs}
\end{equation}
where $\Gamma$ is the max number of concurrent processes \emph{in the cluster}. 
Note that $\frac{\Delta}{\Gamma}$ will usually be much lower than 
$\frac{\delta}{\gamma}$.     


% KEEP THIS FOR WHEN WE LOOK AT SIMULATION

%\subsection{Simulation} % is it included? 0.5 page

% \todo{Remove if not included}

% Even if we end up not using simulation, here we should explain
% why the model in simgrid is limited. Refer to Fred's paper at CCGrid 2017.
% The following sub-sections become a set of models that we could implement
% in simulation to make it more realistic.

% Talk about ~\cite{lebre2015adding} and \cite{wrench}. Most simulation toolkits have focused on task 
%  scheduling. And https://dl.acm.org/citation.cfm?id=3041715

\subsection{Infrastructure} % 1/4 a page (half a col)

 All experiments were executed on 
 \href{https://www.dellemc.com/resources/en-us/asset/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf}{Dell 
 EMC's Zenith cluster}, a Top-500 machine in the 
 \href{https://www.dellemc.com/en-us/solutions/high-performance-computing/HPC-AI-Innovation-Lab.htm}{Dell 
 EMC HPC and AI Innovation Lab}, running Slurm. For the Spark 
 experiments, a Spark cluster was started on our 
 Slurm allocation through access to 16 dedicated nodes. Each compute 
 node ran on RHEL 7.4, was 
 equipped with 40 Intel Xeon Gold 6148 2.4GHz CPUs and had
 187GB of memory, 94GB of tmpfs and a Dell 120GB M.2 SATA SSD as local disk. 
 A 698TB Lustre server was accessible on each node through a 100Gb/s 
 Infiniband network. The apparent write bandwidth of the local disk, 
 Lustre file system and tmpfs were measured by sequentially writing various numbers
 of image blocks containing random intensities, to avoid 
 caching effects (measure\_bandwidth.py script in folder 
 benchmark\_scripts). They are reported in Table~\ref{table:bdwdths}.

\begin{table}
\centering
\begin{tabular}{c|c}
\rowcolor{headcolor}
Data location & Measured write bandwidths (MB/s)\\
\hline
tmpfs                 & 1377.18 \\
Local disk ($\delta$) & 193.64  \\
Lustre   ($\Delta$)   & 504.03 \\
\end{tabular}
\caption{Measured bandwidths}
\label{table:bdwdths}
\end{table}

\subsection{Datasets} % 1/4 of a page (half a col)

We used BigBrain~\cite{amunts2013bigbrain}, a 75GB 40$\mu$m isotropic 
histological image of a 65-year-old male's brain. The BigBrain was selected due 
to being the only one of its kind, as there does not yet exist any other
higher-resolution image of a human brain. Moreover, there currently exists a 
lack of standardized tools for processing the BigBrain as a consequence 
of its size. To examine the effects processing the BigBrain has on Big Data 
strategies, we partitioned the full $3845\times3015\times3470$ voxel image into 30 ($5\times3\times2$)
chunks, 125 ($5\times5\times5$) chunks and 750 ($5\times15\times10$) chunks. Additionally, the full 
image was also split in half ($769\times603\times347$ voxels) and the half image was 
partitioned into 125 chunks.

Processing large images is only considered to be part of the Big Data problem 
in neuroscience. The other problem is processing large MRI datasets, 
that is, datasets consisting of many small brain images belonging to 
various different subjects. This situation is commonly observed in 
functional MRI (fMRI), where it is becoming increasingly common to 
process data from hundreds of subjects. Although we have not explored 
explicitly the processing of large MRI datasets, the 75GB BigBrain is 
within the size ballpark~\cite{van2014human} of 
MRI datasets commonly processed in today's studies.

As it is possible that both small and large data may need to be processed using
the same analysis pipeline, we 
wanted to determine the effects of the data management strategies on small 
data as well. For this, we selected a 12MB T1W image belonging to 
subject 1 of OpenNeuro's 
\href{https://openneuro.org/datasets/ds000001/versions/00006/file-display/sub-01:anat:sub-01_T1w.nii.gz}{ds000001 
dataset version 6}. In order to be able to split it into 125 chunks, it 
was necessary to zero-pad the image to the dimensions
$165\times200\times200$ voxels, which subsequently increased the total image size to 13MB.

\subsection{Applications} % 1 page
\begin{algorithm}\caption{Incrementation}\label{alg:incrementation}
    \begin{algorithmic}[1]
    \Input
    \Desc{$x$}{a sleep delay in seconds}
    \Desc{$n$}{a number of iterations}
    \Desc{$C$}{a set of image chunks}
    \Desc{$fs$}{filesystem to write to (mem, tmpfs, local disk, Lustre).}
    \EndInput
    \ForEach{$chunk \in C$}
    \State read $chunk$ from Lustre
    \For{$i \in [1, n]$}
        \State $chunk\gets chunk+1$
        \State sleep $x$
        \If{$i < n$}
        \State save $chunk$ to $fs$
        \EndIf
    \EndFor
    \State save $chunk$ to Lustre
    \EndFor
\end{algorithmic}
\end{algorithm}

In order to investigate how the different strategies impact processing, we 
selected a simple incrementation pipeline (Algorithm~\ref{alg:incrementation}) 
that consisted exclusively of map 
stages. A series of map-only stages would enable us to evaluate the effects of
in-memory computing when data locality is preserved. Incrementation was 
selected 
over other applications, such as binarization, as it ensured that a new image
was created at each step (i.e. no caching effects within the executing 
application). Each partitioned chunk was incremented by 1, in parallel, by
the applications. As incrementing the images is a relatively quick 
process, we added a sleep delay to the incrementation stage in order to 
increase task duration as required. The incremented chunks would be either 
maintained in-memory (Spark only) or saved to either tmpfs, local disk or 
Lustre (Spark and Nipype). Should more than a single iteration be requested, 
the incremented chunks would be incremented again and saved to the same storage. 
This would repeat until the number of requested iterations had elapsed. In all 
conditions, the first input chunks and final output chunks would be read from / written to  Lustre. 
We chose to perform our initial 
reads and final writes on Lustre as a multi-tenant clusters' compute node's 
local storage, if 
available, is generally temporary and only available for the duration of the 
allocation. 

\subsection{Experiments}

We conducted four experiments in which we varied (1) the number of 
iterations in the application ($n$ in 
Algorithm~\ref{alg:incrementation}), (2) the task duration ($x$), (3) 
the chunk size for a constant image size, (4) the total image size. To 
evaluate the page-cache model, experiment conditions fell in different 
regions of Equations~\ref{eq:page-cache-inequality} 
and~\ref{eq:page-cache-sharedfs}, as summarized in 
Table~\ref{table:experiments}. Among the 16 nodes available, 1 was 
dedicated to the Spark master and driver, and the remaining 15 were 
used as compute nodes. Since data-locality is not normally preserved in 
Nipype (a new Slurm allocation is requested for each processed task), 
we instrumented Nipype to ensure data locality \todo{You could refer to the script doing that in the paper repo.}. That is, the chunk were split into partitions, and for each partition of 
chunks, we requested a Slurm allocation to process the entire 
pipeline in parallel, using Nipype's MultiProc scheduler, on a given 
node. This was possible as no communication was required between the 
processed chunks.

\begin{table*}
\centering
\begin{tabular}{c|ccc|cccc|cc}
  \rowcolor{headcolor}
  \multicolumn{10}{c}{Experiment 1: Number of Iterations}\\
  \hline
  \rowcolor{headcolor}
 n &
 D (GB) & C (s) & D/C (MB/s) &
 $\gamma$ & $\delta/\gamma$ (MB/s) & $\Gamma$ & $\Delta/\Gamma$ (MB/s)&
 (D/C)/($\delta/\gamma$) & (D/C)/($\Delta/\Gamma$)\\
 \hline
1   &75    & 430    & 178.6 & 9 & 21.5 & 125 & 4.0 & \cellcolor{red!20} 8.3 & \cellcolor{red!20} 44.7\\
10  & 750   & 4,300 & 178.6 & 9 & 21.5 & 125 & 4.0 & \cellcolor{red!20} 8.3 & \cellcolor{red!20} 44.7\\
100 & 7,500 & 43,000& 178.6 & 9 & 21.5 & 125 & 4.0 & \cellcolor{red!20} 8.3 & \cellcolor{red!20} 44.7\\
\hline
  \multicolumn{10}{c}{}\\

\rowcolor{headcolor}
  \multicolumn{10}{c}{Experiment 2: Task Duration}\\
  \hline
  \rowcolor{headcolor}
 x (s) &
 D (GB) & C (s) & D/C (MB/s) &
 $\gamma$ & $\delta/\gamma$ (MB/s) & $\Gamma$ & $\Delta/\Gamma$ (MB/s)&
 (D/C)/($\delta/\gamma$) & (D/C)/($\Delta/\Gamma$)\\
 \hline
 2.4  & 750 & 3,000   & 256   & 9 & 21.5 & 125 & 4.0 &  \cellcolor{red!20} 11.9 & \cellcolor{red!20} 64   \\
 3.44 & 750 & 4,300   & 178.6 & 9 & 21.5 & 125 & 4.0 &  \cellcolor{red!20} 8.3 & \cellcolor{red!20} 44.7   \\
 7.68 & 750 & 9,600   & 80    & 9 & 21.5 & 125 & 4.0 &  \cellcolor{red!20} 3.7 & \cellcolor{red!20} 20   \\
 320  & 750 & 400,000 & 1.9   & 9 & 21.5 & 125 & 4.0 &  \cellcolor{green!20} 0.09 & \cellcolor{green!20} 0.48  \\
 \hline
  \multicolumn{10}{c}{}\\
  
 \rowcolor{headcolor}
  \multicolumn{10}{c}{Experiment 3: Number of Chunks}\\
  \hline
  \rowcolor{headcolor}
 chunks &
 D (GB) & C (s) & D/C (MB/s) &
 $\gamma$ & $\delta/\gamma$ (MB/s) & $\Gamma$ & $\Delta/\Gamma$ (MB/s)&
 (D/C)/($\delta/\gamma$) & (D/C)/($\Delta/\Gamma$)\\
 \hline
 30  & 750 & 4,400   & 174.6 & 2  & 96.8 & 30  & 16.8 &  \cellcolor{red!20} 1.8 & \cellcolor{red!20} 10.4   \\
 125 & 750 & 4,400   & 174.6 & 9  & 21.5 & 125 & 4.0 &  \cellcolor{red!20} 8.1 & \cellcolor{red!20} 43.7  \\
 750 & 750 & 4,400   & 174.6 & 25 & 7.7  & 375 & 1.3 &  \cellcolor{red!20} 22.7 & \cellcolor{red!20} 134.3  \\

\hline
  \multicolumn{10}{c}{}\\ 
 \rowcolor{headcolor}
  \multicolumn{10}{c}{Experiment 4: Image Size}\\
  \hline
  \rowcolor{headcolor}
 image  &
 D (GB) & C (s) & D/C (MB/s) &
 $\gamma$ & $\delta/\gamma$ (MB/s) & $\Gamma$ & $\Delta/\Gamma$ (MB/s)&
(D/C)/($\delta/\gamma$) & (D/C)/($\Delta/\Gamma$)\\
 \hline
 BigBrain      & 750   & 2,200   & 349.1     & 9  & 21.5  & 125 & 4.0 &  \cellcolor{red!20} 16.2 & \cellcolor{red!20} 87.3   \\
 Half BigBrain & 375   & 2,200   & 174.6     & 9  & 21.5  & 125 & 4.0 &  \cellcolor{red!20} 8.1 & \cellcolor{red!20} 43.7   \\
 MRI           & 0.127 & 2,200   & 0.06      & 9  & 21.5  & 125 & 4.0 &  \cellcolor{green!20} 0.003 & \cellcolor{green!20} 0.015  
\end{tabular}
\caption{Experiment conditions. Red cells denote the conditions where 
the inequalities in Equations~\ref{eq:page-cache-inequality} 
and~\ref{eq:page-cache-sharedfs} do not hold, i.e., the page cache is 
asymptotically useless. Green cells show the conditions where the page cache covers all data writes.}
\label{table:experiments}
\end{table*}

For our first incrementation experiment, we investigated the effects of 
Big Data strategies on varying total data size. To do this, we 
increased the number of incrementation iterations from 1, 10 and 100 
times. The total data size would then increase from 75GB, at 1 
iteration, to 7,500GB, at 100 iterations. The total number of chunks 
was 125. Chunks all ran concurrently ($\Gamma$=125) and they were 
equally balanced among 15 nodes, leading to 8 or 9 concurrent jobs per 
node ($\gamma$=9). Task duration was fixed at 3.44 seconds.

In the second experiment, we evaluated the effects of Big Data 
strategies on varying task duration. If the page cache has sufficient 
time to flush, it should be expected that using in-memory computing 
should have no increased speed-up over disk. We varied the task 
duration between 2.4 seconds and 320 seconds so that the D/C falls into 
different regions of Equations~\ref{eq:page-cache-inequality} 
and~\ref{eq:page-cache-sharedfs}. The number of chunks was maintained 
at 125, leading to $\Gamma$=125 and $\gamma$=9. The number of iterations was
fixed to 10.

As a third incrementation experiment, we were interested in the effects of 
chunk size on Big Data strategies. Naturally, greater chunk size signifies 
a decrease in parallelization, however, it also signifies an increase in 
sequential I/O (increased $\Delta/\Gamma$ and $\delta/\gamma$). For 
this experiment we partitioned the complete BigBrain image into 30, 125 
and 750 chunks, corresponding to $\gamma$ values of 2, 9 and 25 
respectively. While Spark attempted to load-balance the data, it used 
up only 25 of the 40 cores for 750 chunks. In contrast, Nipype tried to 
use up as many cores as possible. Unlike the previous 
experiment, the D/C ratio was kept static at 178.6MB/s, however, this 
ratio ensured that different regions of the inequality were reached 
depending on amount of parallelization. Unfortunately, 30 chunks could 
no be processed using in-memory Spark as its individual chunk size of 
3GB exceeded the 2GB partition size limit imposed by Spark \todo{You should move this comment to the results or the reader will wonder
what happened to the yellow bar when looking at figure 1.c.}. The number of iterations was
fixed to 10, and the task duration was adjusted so that C remained constant at 4,400s.

For our fourth and final incrementation experiment, we investigated the effects 
of the strategies on different image sizes. We selected the 75GB BigBrain, the
38GB half BigBrain and the 13M T1W MRI image for this experiment. The number of
chunks was fixed at 125. Similarly to the previous experiment, the 
total sequential compute time was fixed (10 iterations, 1.76 seconds 
per task), however, due to varying size in total data processed (D), 
the D/C ratio varied. Once again, we ensured that the D/C ratio fell in 
multiple different regions of the inequality. The D/C ratio ranged from 
349.1 MB/s for BigBrain and 174.6MB/s for half Bigbrain, to 0.06MB/s for the MRI 
image. Only the 0.06MB/s MRI satisfied the inequality for both Lustre and local 
disk.


% average (1 shuffle)
% Nipype: can't use SLURM plugin so had to actively poll.

% kmeans (many shuffles)

% Example BIDS app

% fmriprep?

% Explain how we tweaked scheduling in Nipype to make it similar to Spark, 
% otherwise we're only measuring scheduling differences. Needs a discussion on scheduling somewhere.

% Talk about our I/O pattern: random I/O, sequential.


\section{Results} % 2 pages
\label{sec:results}

% We checked that scheduling was similar in both cases (show a two Gantt charts to illustrate)

\begin{figure*}
\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{results/figures/iterations.pdf}
    \caption{Experiment 1: complete BigBrain, 125 chunks, 3.44-second tasks.}\label{fig:iterations}
\end{subfigure}
\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{results/figures/cputime.pdf}
    \caption{Experiment 2: complete BigBrain, 125 chunks, 10 iterations.}\label{fig:cputime}
\end{subfigure}
\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{results/figures/numchunks.pdf}
    \caption{Experiment 3: complete BigBrain, 10 iterations, C= 4,400s.}\label{fig:numchunks}
\end{subfigure}
\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{results/figures/datasize.pdf}
    \caption{Experiment 4: 125 chunks, 10 iterations, 1.76-second tasks.}\label{fig:datasize}
\end{subfigure}
\caption{Experiment results: Makespans of Spark and Nipype writing to memory, tmpfs, local  disk and Lustre.}
\label{fig:results}
\end{figure*}
\subsection{Experiment 1: Number of Iterations}

Fig.~\ref{fig:iterations} shows the difference between the different 
filesystem choices given the number of iterations. At 1 iteration, all 
filesystems behave the same, although the application was writing faster than the page cache flushed. \todo{Update the graph to the same y scale and 'broken'
bars, and check that this impression still holds...}
This is explained by the fact that the application data was not large enough to saturate the page  
cache fully (transient phase). As the total amount of data written by the 
application increases to 750~GB, there is a greater disparity between 
Lustre and in-memory (2.67~x slower, on average). Local disk 
performance, however, 
is still comparable to memory (1.38~x slower, on average). Despite local disk and 
Lustre both being in transient state, local disk encounters less contention 
than what would be found on Lustre. 

%sp_mem 1it = 34s, 10it = 64 100it = 381
%3.44 * 1 = 3.44, 3.44*10 = 34, 3.44*100 = 344

%sp_tmpfs 1it = 31, 10it = 69, 

At 100 iterations, or 7,500~GB, Lustre can be found to be, on average, 3.82~x 
slower than Spark in-memory. The page cache, on Zenith, occupies 20\% of 
total memory. Since there was 187~GB of RAM on each node, only 37.4~GB of 
dirty data could be held in a node's page cache at any given time. The 
slowdown experienced can be explained by the smaller percentage of total data 
residing in the page cache at a given time, compared to 10 iterations. 
Therefore, the effects of Lustre bandwidth are more significant in 
this application. At 100 iterations, the application was writing 500GB 
per node (7,500 GB / 15 nodes) and hence could not run on tmpfs or local disk.

While there is some variability that can be seen in Fig.~\ref{fig:iterations} 
between the two engines, this believed to be insignificant, and potentially due 
to SLURM node allocation delays in our launching of Nipype.

% Comment on the general trend
% note 'on average' = average between spark and nipype. local disk spark is 
% 1.22x slower and local disk nipype is 1.54x slower
% spark lustre 2.66x slower; nipype lustre 2.68x slower

\subsection{Experiment 2: Task Duration}
% Comment on the general trend
% We can't really say that imaging tasks are not small. Even Freesurfer could be decomposed.
%

Increasing task duration ensured that all file systems had a comparable performance
(Fig.~\ref{fig:cputime}). Lustre, for instance, is approximately 1.01x slower
than Spark in-memory at a task duration of 320 seconds, whereas it is 
approximately 3.25x slower that Spark in-memory with 2.4 second tasks. This 
pattern corroborated our page-cache model which postulates that 
data movement costs will have little impact on compute-intensive tasks. The 
reasoning behind this is that longer tasks give the page cache more time to flush 
between disk writes. The longer the delay, the less likely the page cache is 
to fill up when the concurrent data size written fits within the page cache.

\subsection{Experiment 3: Image Block Size}

% Comment on the general trend
As can be seen in Fig.~\ref{fig:numchunks}, makespan decreases when the 
number of chunks increases. This is due to the fact that parallelism 
increases with an increase in the number of chunks. With 30 chunks, 
only 2 CPUs per node are actively working. At 125 chunks, this changes 
to a maximum of 9 CPUs per node, and at 750 chunks all 40 CPUs can be 
active. It can be noted that no significant speedup can be seen between 125 
chunks and 750 chunks \todo{This is not true for in-memory, Spark tmpfs and Spark local disk}. This is because out of the 50 chunks processed by each 
node, only 40 can be processed concurrently, thus requiring two sequential 
incrementation batches and resulting in similar makespans as 125 
chunks.

In terms of filesystem selection, local and tmpfs perform comparably for all 
conditions, with Lustre being significantly slower. As with varying the number 
of iterations, Lustre is slower due to increased filesystem contention, which 
is, at minimum, 15x greater than contention on local disk. With an increase in 
number of chunks, local disk and tmpfs makespans begin to converge. A potential 
explanation for this may be that tmpfs is utilizing swap space. As concurrency 
increases, the memory footprint of the application also increases. It is 
possible that at 750 chunks, swapping to disk is required by tmpfs, thus 
resulting in similar processing times as local disk.

Swapping may also be an explanation for the variance between Spark in-memory 
and tmpfs performance. While Spark may also spill to disk, it only does so when
data does not fit in memory. As none of the RDDs generated throughout the 
pipeline were cached and all data concurrently accessed could be mantained 
in-memory, even for the 750 chunks which were processed in two batches, it did 
not need to spill to disk.

\subsection{Experiment 4: Image Size}

% Comment on the general trend
Increasing overall data size decreases performance, as can be seen in 
Fig.~\ref{fig:datasize}. When the data size is very small, as is the case 
with the MRI image, all file system makespans are comparable. This is due to the 
fact that page cache can be leveraged fully regardless of file system. However, 
this time, Spark in-memory performed significantly worse than all other 
filesystems, with a makespan of 2,211 seconds (not reported on the graph to preserve the y scale) \todo{You may want to add it back when you have
broken bars}. Upon further inspection, it appeared that Spark in-memory executed
in a sequential order, on a single worker node. Lack of parallelism for the MRI 
image may be a result of Spark's max partition size, which is by default 128~MB
-- significantly larger than the 13~MB MRI image. 

At half BigBrain, the makespan differences become apparent in both local disk 
and Lustre, with Lustre becoming 2.4x slower than in-memory. This 
can be attributed to page cache saturation, as predicted by the model for both 
half the BigBrain image and the complete BigBrain. Only the MRI image was 
predicted to fall within the model constraints. 

When the complete BigBrain is processed, the disparity between the different 
filesystems becomes even greater. Lustre becomes 3.68x slower, whereas local 
disk is now 1.68x slower. The reasoning for this is that the page cache fills 
up faster than with half of BigBrain.

\todo{change Spark's max partition size and test to see if it fixes the problem}

\subsection{Page Cache Model Evaluations}
\begin{figure*}
\begin{subfigure}{0.5\linewidth}
\centering
    \includegraphics[width=\textwidth]{results/figures/local-incrementation.pdf}
\caption{Local Disk}
\label{fig:modeleval-local}
    \end{subfigure}%
\begin{subfigure}{0.5\linewidth}
\centering
    \includegraphics[width=\textwidth]{results/figures/lustre-incrementation.pdf}
\caption{Lustre}
\label{fig:modeleval-lustre}
\end{subfigure}
    \caption{Page cache model evaluation. Grey 
             regions denote areas that violate model predictions.}
\label{fig:modeleval}
\end{figure*}

In order to evaluate the page cache model, we compared the observed 
speedup ratio provided by in-memory computing to the (D/C) / 
($\delta$/$\gamma$) and (D/C) / ($\Delta$/$\Gamma$) ratios 
(Fig.~\ref{fig:modeleval}). Speed-up ratios were computed as the ratio 
between the makespan obtained with Spark on local disk or Lustre, and 
the makespan obtained with Spark for in-memory computing. Experiments 
for which there was no in-memory equivalent (i.e. BigBrain split into 
30 
chunks) 
were not considered.

%probably confusing to mention values, but they're here for now
Results show that, overall, the model correctly predicted the effect of 
page cache on processing times for local disk and Lustre. That is, the 
speed-up provided by in-memory computing was larger than 1 for D/C 
ratios larger than $\delta/\gamma$ (local disk) or $\Delta/\Gamma$ 
(Lustre). Conversely, the speed-up provided by in-memory computing 
remained close to 1 for D/C ratios smaller than $\delta/\gamma$ (local 
disk) or $\Delta/\Gamma$ (Lustre). The two points close to the origin 
correspond to the sequential processing of the MRI image by Spark 
mentioned previously.

Points which violated 
model predictions were found at 1 iteration, where page cache would not 
have been saturated in spite of a high D/C (transient state). However, 
in all cases, the ``1" boundary was never trespassed by more than a 
factor of 0.19, and is therefore likely a result of system variability.

\section{Discussion} % 2 pages
\label{sec:discussion}

\subsection{Effect of In-Memory Computing}
% Shared fs vs local disk vs in-memory: what do we gain?
We measure the effect of in-memory computing by comparing the runs 
of Spark in-memory (yellow bars in Fig.~\ref{fig:results}) to the ones 
of Spark on local disk (non-hatched green bars). The speed-up provided 
by in-memory computing is also reported in 
Fig.~\ref{fig:modeleval-local}. The speed-up provided by in-memory 
computing increases with (D/C) / ($\delta$/$\gamma$), as expected from 
the model. In our experiments, it peaked at 1.6, for a ratio of 16.2. 
This correspond to the processing of the BigBrain with 125 chunks and 
1.76-second tasks in experiment 4 (total computing time C=2,200s), 
which is typically encountered in common image processing tasks such as 
denoising, intensity normalization, etc. The speed up of 1.6 is also 
reached with a ratio of 22.7 in experiment 3, obtained by processing 
the BigBrain with 750 chunks.

The results also allow us to speculate on the effect of in-memory 
computing on the pre-processing of functional MRI, another typical use 
case in neuroimaging. Assuming an average processing time of 20 minutes 
per subject, which is a ballpark value commonly observed with the 
popular SPM or FSL packages \todo{add refs}, an input data size of 100MB per subject, 
and an output data size of 2GB (20-fold increase compared to input 
size), the D/C ratio would be 1.8MB/s, which would reach the 
$\delta/\gamma$ threshold measured on this cluster for $\gamma=108$, 
that is, if 108 subjects were processed on the same node. This is very 
unlikely as the number of CPUs per node is 40. We therefore conclude 
that in-memory computing is likely to be useless for fMRI analysis.
Naturally, this estimate is strongly dependent on the characteristics of the cluster.

\subsection{Effect of Data Locality}

We measure the effect of data locality by comparing the runs of Spark
on local disk (non-hatched green bars in Fig.~\ref{fig:results}) to the 
ones of Spark on Lustre (non-hatched blue bars). The speed-up provided by 
local execution peaks at 3.2, reached at 750 chunks in experiment 3. Overall, 
writing locally was usually preferrable to writing to Lustre, as a result of the
lower contention on local disk. Although it may be true, as stated in~\cite{Anantharayanan2011datalocality},
network bandwidths generally exceed that of disks, locality remains important 
as contention to a shared filesystem will be much higher than local disk.
The only time in our experiments where it was found that writing locally did not
have a huge impact over Lustre, was in experiments 1 and 4, at 1 iteration and 
when processing the MRI image, respectively. In both these scenarios, the Lustre 
writes did not impact performance as the data was able to be written to page cache
and flushed to Lustre asynchronously.

\subsection{Combined Effect of In-Memory and Data Locality}
We measure the combined effect of data locality and in-memory computing 
by comparing the runs of Spark in-memory (yellow bars in 
Fig.~\ref{fig:results}) to the ones of Spark on Lustre (non-hatched 
blue bars). The speed-up provided by the combine use of data locality 
and in-memory computing is also reported in 
Fig.~\ref{fig:modeleval-lustre}. The provided speed-up increases with 
(D/C) / ($\Delta$/$\Gamma$), as expected from the model. In our 
experiments, it peaked around 5, for ratios of 120.4 and 64. Again, 
this configuration is likely to happen in typical image processing 
tasks performed on the BigBrain.

As for the fMRI speculation, the D/C 
ratio of 1.8MB/s would reach the $\Delta/\Gamma$ threshold for 
$\Gamma=280$, which is a realistic number of subjects to process on a 
complete cluster. Naturally, this estimate is highly dependent on the observed bandwidth
of the shared file system ($\Delta$).

\subsection{Effect of Lazy Evaluation}

The effects of lazy-evaluation can be seen throughout the experiments. Nipype 
was found to generally be slower than Spark in all experiments. While a Nipype DAG is 
generated prior to workflow execution, there are no such optimizations, as in 
Spark, to ensure that the least amount of data transfer is performed to produce the 
results. 

During the processing of Experiment 3, it was found that Spark split up the data
into an processed it in two batches for the 750 chunk input. That is to say, 
each node in the cluster processed 50 chunks, and since there was only a maximum
of 40 cores available on each node, Spark opted to run \todo{a word is missing here} to batches of 25 to ensure
a balanced workload. Rather than running each iteration on the full dataset, as with Nipype, 
Spark opted to perform all the iterations on the first batch (load, increment, save),
and then proceeds to process the second batch. Such an optimization is important, 
even when processing data on disk, as it would presumably increase the occurrence
of cache hits. This may partially explain the speedup seen at 750 chunks in
Figure~\ref{fig:numchunks}.% Spark local vs nipype local: presumably increases cache hits
\todo{Add as sentence on this paragraph in the abstract.}

\subsection{Can tmpfs and Page Caches Emulate In-Memory Computing?}
% tmpfs and local disks fill up: need cleanup.
% Can we emulate in-memory computing using write buffers or tmpfs?

Although tmpfs and page cache do improve performance in general, as seen in
Figure~\ref{fig:results}, they do not always perform equivalently to in-memory. 
Tmpfs's main limitation is that data residing on it may be swapped to disk if 
the system's memory usage is high. When it reaches this point, its performance 
slows down to swap disk bandwidth, as observed in Figure~\ref{fig:datasize}. Page cache suffers a similar dilemma. As detailed
in~\href{https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/performance_tuning_guide/s-memory-tunables}{RedHatDoc}\todo{add to references},
I/O blocking write to disk occurs when a giving percentage (e.g. 20~\%) of total memory is occupied by dirty data, once this 
threshold is exceeded, processes wanting to execute writes must wait until the dirty data
in the cache is flushed to disk \todo{Revise this sentence (grammar?)}.

Furthermore, like memory, tmpfs and page cache are shared resources on a node.
If users on the node are heavily using memory to incite tmpfs to write to swap space, or are performing data-intensive operations
that fill up the page cache, this will limit the performance on tmpfs/page cache of other users. 
However, with memory, it is possible to request, through the HPC scheduler, a required
amount of available memory. Ultimately, in-memory data will also need to be spilled to
disk if memory usage exceeds amount of available memory, although disk writes are 
likely to occur in tmpfs and page cache before requested available memory is filled.

\subsection{Scheduling Remarks}

A common recommendation in Spark is to limit the number of cores per 
executors to 5, to preserve a good I/O 
throughput\footnote{\url{http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2}}, 
but the rationale for this recommendation is hardly explained. We 
believe that throughput degradation observed with more than 5 cores per 
executor might be coming from full page caches.

Spark currently doesn't include any active management of disk I/Os or 
page caches. We believe that it would be beneficial to extend it toward 
this direction, to increase the performance of operations where local 
storage has to be used, such as disk spills or shuffles. For instance, 
workflow-aware cache eviction policies that maximizes page cache usage 
for the workflows could be investigated. 

An alternative Nipype plugin designed for running on Slurm was not used 
in the experiments. The Slurm plugin requests a Slurm allocation for 
each processed data chunk. Such a scheduling strategy was not ideal in 
our environment where oversubscription of nodes was not enabled.

Unlike Spark, Nipype, by default, opts to use all available CPUs rather 
than to load balance data across the cluster. That is, given 50 chunks 
and 40 cores, Spark will only use up 25 cores, and will break it up 
into two batches. Nipype, on the other hand, will also have no choice 
but to split up the processing into two batches, but will first process 
40 chunks, immediately followed by the remaining 10. While both are 
reasonable strategies for data processing, Spark may end up benefiting 
more from the page cache, as less data is written in parallel (25 vs 
40), giving more time for the page cache to flush.

With Nipype, MapNodes, which apply a given function to each element in 
a list, were found to be slower than the Node, which apply a function 
to a single element, due to a blocking mechanism. For this reason, we 
selected to iterate through a series of Nodes in our code \todo{while 
MapNodes might have been easier to use?}.

\todo{If time permits, it would be useful to say that we checked that scheduling between 
both engines was similar, and show two Gantt charts to justify that.}

\subsection{Other Comments}

Writing to node-local storage in a cluster environment comes at a cost, for both
Nipype and Spark without HDFS. When a node is lost, the node-local data is lost
with it. While Spark will recompute the lost partition automatically using lineage, 
Nipype will fail for all tasks requiring the data. Spark, however, will fail if 
RDDs of node-local filenames are shuffled, as the data associated with the filenames
will not be shuffled with the RDD

Nipype, when executed on Lustre, will checkpoint itself, ensuring resumption from
last checkpoint. This is particularly important in the case of compute-intensive 
applications, such as those found in neuroimaging. Spark also provides a checkpointing
mechanism, however, it requires HDFS.

It is common in neuroimaging applications for users to want access to all intermediate
data. Such a feature is currently only possible when writing to shared filesystem. 
It would also not be an option with Spark in-memory. To enable this, burst buffers or 
heteorogeneous storage managers (e.g. Triple-H~\cite{tripleH}) could be used to ensure
fast processing and that all outputs (including intermediate outputs) will be 
sent asynchronously to the shared filesystem. \todo{double check triple-H}

It was expected that Spark would experience longer processing times, particularly
with the small datasets, due to Java serializing. This was not found to be the case. 
Unlike Spark, Nipype performs a more thorough provenance capture, potentially owing to longer
processing times.

\section{Conclusion} % 1 page with refs
\label{sec:conclusion}

Big Data performance optimization strategies help improve performance of typical 
neuroimaging applications. Our experiments indicate that overall, in-memory computing 
enables greater speedups than what can be obtained by using page cache 
and tmpfs. While page cache and tmpfs do give memory-like performance, 
they are likely to fill up faster than memory, leading to increased 
performance penalties when compared to in-memory computing. We conclude 
that extending Big Data processing engines to better support 
neuroimaging applications, including developing their provenance, 
fault-tolerance, and reproducibility features, is a worthwhile project.

Data-locality was also found to play an important role in application 
performance. Local disk was found to perform between than the shared 
filesystem \todo{between than the shared filesystem: what does it 
mean?}, despite having lower bandwidth, due to increased contention on 
the shared filesystem. As local disk will generally have less 
contention than a shared filesystem. It is recommended to have the data 
stored locally. Although, storing data locally may come at a cost to 
fault tolerance.

Although a more thorough analysis of lazy evaluation remains to be 
performed, it is speculated that this may be the cause of the general 
performance difference between Spark and Nipype. Furthermore, it is 
found that lazy evaluation optimizations increase the likelihood of 
cache hits, thus improving overall performance.

Even though Big Data strategies are beneficial to the processing of 
large images, it is estimated that it would require running a 
functional MRI dataset with 280 concurrent subjects for any noticeable 
impact using our Lustre bandwidth estimate. Benchmarking Spark and 
Nipype using such a large fMRI dataset would be a relevant follow-up 
experiment, to test this hypothesis. It would also be useful to evaluate
other types of applications, such as ones containing data shuffling steps.

Finally, we plan to extend this study by including task scheduling 
strategies in a multi-tenant environment. We expect to observe 
important differences between Spark and Nipype, due to Spark's use of 
overlay scheduling. The impact of other Big Data technologies, such as 
distributed in-memory file systems~\cite{ignite}, could also be 
investigated.

\section{Acknowledgments}

We are thankful to the Dell EMC HPC and AI Innovation Lab
in Roundrock, TX, for providing the infrastructure and high-quality
technical support that made possible the experiments reported in this paper.

\bibliographystyle{IEEEtran} 
\bibliography{biblio}

\end{document}























